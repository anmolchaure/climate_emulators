{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93283,"status":"ok","timestamp":1688396653588,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"0FUA4Z5wSGrW","outputId":"2b83ff14-ed3f-4533-fa73-5462b7d5acce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gpflow\n","  Downloading gpflow-2.8.1-py3-none-any.whl (376 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.8/376.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting check-shapes>=1.0.0 (from gpflow)\n","  Downloading check_shapes-1.0.0-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting deprecated (from gpflow)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: multipledispatch>=0.6 in /usr/local/lib/python3.10/dist-packages (from gpflow) (0.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpflow) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gpflow) (23.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpflow) (1.10.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from gpflow) (67.7.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from gpflow) (0.8.10)\n","Requirement already satisfied: tensorflow-probability>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gpflow) (0.20.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gpflow) (4.6.3)\n","Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from gpflow) (2.12.0)\n","Collecting lark<2.0.0,>=1.1.0 (from check-shapes>=1.0.0->gpflow)\n","  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from multipledispatch>=0.6->gpflow) (1.16.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.56.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.4.10)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (3.20.3)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (2.3.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow) (0.32.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow) (4.4.2)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow) (2.2.1)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow) (0.1.8)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.0->gpflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.4.0->gpflow) (0.2.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (2.3.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.4.0->gpflow) (3.2.2)\n","Installing collected packages: lark, deprecated, check-shapes, gpflow\n","Successfully installed check-shapes-1.0.0 deprecated-1.2.14 gpflow-2.8.1 lark-1.1.5\n","Collecting utils\n","  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n","Installing collected packages: utils\n","Successfully installed utils-1.0.1\n","Collecting cartopy\n","  Downloading Cartopy-0.21.1.tar.gz (10.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from cartopy) (1.22.4)\n","Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.10/dist-packages (from cartopy) (3.7.1)\n","Requirement already satisfied: shapely>=1.6.4 in /usr/local/lib/python3.10/dist-packages (from cartopy) (2.0.1)\n","Collecting pyshp>=2.1 (from cartopy)\n","  Downloading pyshp-2.3.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyproj>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from cartopy) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (4.40.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1->cartopy) (2.8.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyproj>=3.0.0->cartopy) (2023.5.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1->cartopy) (1.16.0)\n","Building wheels for collected packages: cartopy\n","  Building wheel for cartopy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cartopy: filename=Cartopy-0.21.1-cp310-cp310-linux_x86_64.whl size=11102758 sha256=69371189bba2a6ae8addb031cc5e78db545e850930456da7ecfb83703c9a0371\n","  Stored in directory: /root/.cache/pip/wheels/30/b0/1a/1c1909e00c76653dc4e2ff48555257c0eb2d1698280c8d9955\n","Successfully built cartopy\n","Installing collected packages: pyshp, cartopy\n","Successfully installed cartopy-0.21.1 pyshp-2.3.1\n","E: Unable to locate package python-cartopy\n","Found existing installation: shapely 2.0.1\n","Uninstalling shapely-2.0.1:\n","  Successfully uninstalled shapely-2.0.1\n","Collecting shapely\n","  Downloading shapely-2.0.1.tar.gz (275 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.5/275.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely) (1.22.4)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=shapely-2.0.1-cp310-cp310-linux_x86_64.whl size=969695 sha256=93a2c3dd514e7c9a0ead83f20f776167a4915aab63f40b5e69711c6c0b6395b7\n","  Stored in directory: /root/.cache/pip/wheels/07/bd/06/4e979fa263bca266484ee65f5aab8e6b1c9b20f8caa6f2d7da\n","Successfully built shapely\n","Installing collected packages: shapely\n","Successfully installed shapely-2.0.1\n","Collecting eofs\n","  Downloading eofs-1.4.0.tar.gz (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from eofs) (1.22.4)\n","Building wheels for collected packages: eofs\n","  Building wheel for eofs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for eofs: filename=eofs-1.4.0-py3-none-any.whl size=1100355 sha256=31f589f9c8ac9f1634afd0f96771aff4ffcaa6b2193afd7e01bea21e83c2472d\n","  Stored in directory: /root/.cache/pip/wheels/3d/69/36/c8bd247900ab5ade980a418ac2faed41e08f5006f7c369282f\n","Successfully built eofs\n","Installing collected packages: eofs\n","Successfully installed eofs-1.4.0\n"]}],"source":["!pip install gpflow\n","!pip install utils\n","!pip install cartopy\n","!apt-get -qq install python-cartopy python3-cartopy\n","!pip uninstall -y shapely    # cartopy and shapely aren't friends (early 2020)\n","\n","!pip install shapely --no-binary shapely\n","!pip install eofs"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3871,"status":"ok","timestamp":1688396657455,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"IkF-bgS4kTbo"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xarray as xr\n","from glob import glob\n","\n","import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()\n","from tensorflow import keras\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.regularizers import l1_l2\n","from utils import *\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","%load_ext autoreload\n","%autoreload 2\n","\n","plt.rcParams['savefig.dpi'] = 400\n","plt.rcParams['font.size'] = 13\n","plt.rcParams[\"legend.frameon\"] = False"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357225,"status":"ok","timestamp":1688397014670,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"9q7vELmSkVEz","outputId":"c0e5f28f-dac7-4f15-d041-00acc592214e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","\n","\n","drive.mount('/content/gdrive/', force_remount=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":720,"status":"ok","timestamp":1688397015386,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"rEdKEfJYkWfc"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xarray as xr\n","from glob import glob\n","\n","\n","\n","def make_dir(path):\n","    if os.path.exists(path) is False:\n","        os.makedirs(path)\n","\n","\n","def prepare_predictor(data_sets, data_path,time_reindex=True):\n","    \"\"\"\n","    Args:\n","        data_sets list(str): names of datasets\n","    \"\"\"\n","\n","    # Create training and testing arrays\n","    if isinstance(data_sets, str):\n","        data_sets = [data_sets]\n","\n","    X_all      = []\n","    length_all = []\n","\n","    for file in data_sets:\n","        data = xr.open_dataset(os.path.join(data_path, f\"inputs_{file}.nc\"))\n","        X_all.append(data)\n","        length_all.append(len(data.time))\n","\n","    X = xr.concat(X_all,dim='time')\n","    length_all = np.array(length_all)\n","    # X = xr.concat([xr.open_dataset(data_path + f\"inputs_{file}.nc\") for file in data_sets], dim='time')\n","    if time_reindex:\n","        X = X.assign_coords(time=np.arange(len(X.time)))\n","\n","    return X, length_all\n","\n","def prepare_predictand(data_sets,data_path,time_reindex=True):\n","    if isinstance(data_sets, str):\n","        data_sets = [data_sets]\n","\n","    Y_all = []\n","    length_all = []\n","\n","    for file in data_sets:\n","        data = xr.open_dataset(os.path.join(data_path, f\"outputs_{file}.nc\"))\n","        Y_all.append(data)\n","        length_all.append(len(data.time))\n","\n","    length_all = np.array(length_all)\n","    Y = xr.concat(Y_all,dim='time').mean('member')\n","    # Y = xr.concat([xr.open_dataset(data_path + f\"outputs_{file}.nc\") for file in data_sets], dim='time').mean(\"member\")\n","    Y = Y.rename({'lon':'longitude','lat': 'latitude'}).transpose('time','latitude', 'longitude').drop(['quantile'])\n","    if time_reindex:\n","        Y = Y.assign_coords(time=np.arange(len(Y.time)))\n","\n","    return Y, length_all\n","\n","\n","def get_rmse(truth, pred):\n","    weights = np.cos(np.deg2rad(truth.latitude))\n","    return np.sqrt(((truth-pred)**2).weighted(weights).mean(['latitude', 'longitude'])).data.mean()\n","\n","def plot_history(history):\n","    plt.figure()\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Mean squared error')\n","    plt.plot(history.epoch, np.array(history.history['loss']),\n","           label='Train Loss')\n","    plt.plot(history.epoch, np.array(history.history['val_loss']),\n","           label = 'Val loss')\n","    plt.legend()\n","\n","\n","\n","\n","# Utilities for normalizing the input data\n","def normalize(data, var, meanstd_dict):\n","    mean = meanstd_dict[var][0]\n","    std = meanstd_dict[var][1]\n","    return (data - mean)/std\n","\n","def mean_std_plot(data,color,label,ax):\n","\n","    mean = data.mean(['latitude','longitude'])\n","    std  = data.std(['latitude','longitude'])\n","    yr   = data.time.values\n","\n","    ax.plot(yr,mean,color=color,label=label,linewidth=4)\n","    ax.fill_between(yr,mean+std,mean-std,facecolor=color,alpha=0.4)\n","\n","    return yr, mean"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1688397015387,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"uDRSbh3ikXyt"},"outputs":[],"source":["cwd = os.getcwd()\n","\n","train_path = os.path.join(cwd,'/content/gdrive/MyDrive/climate_emulator_duncan_parris/','train_val1')\n","test_path  = os.path.join(cwd,'/content/gdrive/MyDrive/climate_emulator_duncan_parris/','train_val1')\n","\n","make_dir(train_path)\n","make_dir(test_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":33346,"status":"ok","timestamp":1688397048722,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"MWIY4P-4kZAn"},"outputs":[],"source":["# Training set\n","train_files = [\"historical\", \"ssp585\", \"ssp126\", \"ssp370\",\"hist-GHG\",\"hist-aer\"]\n","X_train_xr, X_length  = prepare_predictor(train_files,train_path)\n","y_train_xr, y_length  = prepare_predictand(train_files,train_path)\n","\n","# Test set\n","X_test_xr, _ = prepare_predictor('ssp245', data_path=test_path,time_reindex=False)\n","y_test_xr, _ = prepare_predictand('ssp245',data_path=test_path,time_reindex=False)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1688397048723,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"1B3M2sJDkaLn","outputId":"1507d034-02b7-4b29-a3e8-800432b1499c"},"outputs":[{"output_type":"stream","name":"stdout","text":["(753, 2) (86, 2)\n"]}],"source":["\n","X_train_df = pd.DataFrame({\"CO2\": X_train_xr[\"CO2\"].data,\n","                           \"CH4\": X_train_xr[\"CH4\"].data\n","                          }, index=X_train_xr[\"CO2\"].coords['time'].data)\n","\n","X_test_df  = pd.DataFrame({\"CO2\": X_test_xr[\"CO2\"].data,\n","                           \"CH4\": X_test_xr[\"CH4\"].data\n","                          }, index=X_test_xr[\"CO2\"].coords['time'].data)\n","\n","# Normalization\n","mean, std = X_train_df.mean(), X_train_df.std()\n","\n","X_train_df   = (X_train_df - mean)/std\n","X_test_df    = (X_test_df - mean)/std\n","\n","X_train = X_train_df.to_numpy()\n","X_test = X_test_df.to_numpy()\n","\n","print(X_train.shape,X_test.shape)"]},{"cell_type":"code","source":[],"metadata":{"id":"CxhMiQ6PANu-","executionInfo":{"status":"ok","timestamp":1688397048724,"user_tz":-330,"elapsed":26,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1688397048725,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"KujadcTPkbNf","outputId":"c7ec5a99-88c0-4f50-c450-6093a02503db"},"outputs":[{"output_type":"stream","name":"stdout","text":["(753, 96, 144, 1) (86, 96, 144, 1)\n"]}],"source":["var  = 'tas'\n","y_train = y_train_xr[var].data\n","y_test  = y_test_xr[var].data\n","\n","mean = y_train.mean()\n","std  = y_train.std()\n","\n","y_train = (y_train - mean)/std\n","y_test  = (y_test - mean)/std\n","\n","y_train = np.expand_dims(y_train, -1).astype(\"float32\")\n","y_test  = np.expand_dims(y_test, -1).astype(\"float32\")\n","\n","n_lat, n_lon = y_train.shape[1], y_train.shape[2]\n","print(y_train.shape,y_test.shape)\n","#OUTPUT_DIM ="]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","import math\n","import matplotlib.pyplot as plt"],"metadata":{"id":"5tcsQqzAU29V","executionInfo":{"status":"ok","timestamp":1688397052761,"user_tz":-330,"elapsed":4058,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["latent_dim = 2   # dimensions of the latent space\n","n_filters  = 32  # number of filters for the convolutional layers\n","n_neurons  = 32  # number of neurons for the Dense layers\n","activation = 'relu' # activation function\n","kernel_size = 4\n","learning_rate = 0.001\n","minibatch_size = 64\n","num_epochs     = 200"],"metadata":{"id":"fCLY3n41-dgE","executionInfo":{"status":"ok","timestamp":1688397052763,"user_tz":-330,"elapsed":11,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Generator model\n","latent_dim = 100  # Set the desired dimension of the latent space\n","n_filters = 32  # Set the number of filters for the convolutional layers\n","\n","decoder_input = Input(shape=(latent_dim,))\n","cond_input = Input(shape=(2,))\n","\n","x = Concatenate(axis=1)([decoder_input, cond_input])\n","x = Dense(12 * 18 * n_filters, activation='relu')(x)\n","x = Reshape((12, 18, n_filters))(x)\n","x = Conv2DTranspose(n_filters, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n","x = Conv2DTranspose(n_filters, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n","x = Conv2DTranspose(n_filters, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n","decoder_output = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n","\n","generator_model = Model([decoder_input, cond_input], decoder_output)\n","\n","# Discriminator model\n","discriminator_input = Input(shape=(96, 144, 1))\n","\n","x = Conv2D(n_filters, (3, 3), activation='relu', strides=(2, 2), padding='same')(discriminator_input)\n","x = Conv2D(n_filters, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n","x = Conv2D(n_filters, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n","x = Flatten()(x)\n","discriminator_output = Dense(1, activation='sigmoid')(x)\n","\n","discriminator_model = Model(discriminator_input, discriminator_output)\n","\n","# GAN model\n","gan_input = [decoder_input, cond_input]\n","gan_output = discriminator_model(generator_model(gan_input))\n","\n","gan_model = Model(gan_input, gan_output)\n","\n","# Compile discriminator model\n","discriminator_model.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","# Freeze discriminator weights during GAN training\n","discriminator_model.trainable = False\n","\n","# Compile GAN model\n","gan_model.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","# Print summary of GAN model\n","gan_model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u32xewELEhPh","executionInfo":{"status":"ok","timestamp":1688397053510,"user_tz":-330,"elapsed":756,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}},"outputId":"6cd291ac-db39-4c41-f144-8be68cfb4735"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 100)]        0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 2)]          0           []                               \n","                                                                                                  \n"," model (Functional)             (None, 96, 144, 1)   739969      ['input_1[0][0]',                \n","                                                                  'input_2[0][0]']                \n","                                                                                                  \n"," model_1 (Functional)           (None, 1)            25729       ['model[0][0]']                  \n","                                                                                                  \n","==================================================================================================\n","Total params: 765,698\n","Trainable params: 739,969\n","Non-trainable params: 25,729\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["discriminator_inputs = Input(shape=(n_lat, n_lon, 1))  # shape: (96, 144, 1)\n","x = Conv2D(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(discriminator_inputs)  # shape: (48, 72, 32)\n","x = Conv2D(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x)  # shape: (24, 36, 32)\n","x = Conv2D(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x)  # shape: (12, 18, 32)\n","x = Flatten()(x)  # shape: (6912,)\n","x = Dense(n_neurons, activation=activation)(x)  # shape: (16,)\n","regression_output = Dense(1)(x)  # shape: (1,)\n","\n","discriminator_model = Model(discriminator_inputs, regression_output, name=\"discriminator\")\n","discriminator_model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JPduafx-QXe","executionInfo":{"status":"ok","timestamp":1688397053512,"user_tz":-330,"elapsed":14,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}},"outputId":"700cf5ba-b400-4107-af10-5e9958f4aaa5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 96, 144, 1)]      0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 48, 72, 32)        544       \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 24, 36, 32)        16416     \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 12, 18, 32)        16416     \n","                                                                 \n"," flatten_1 (Flatten)         (None, 6912)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                221216    \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 254,625\n","Trainable params: 254,625\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XDF9W47NZCko","executionInfo":{"status":"ok","timestamp":1688398486100,"user_tz":-330,"elapsed":442,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","epochs = 100  # Set the number of training epochs\n","batch_size = 32  # Set the batch size for training\n","\n","for epoch in range(epochs):\n","    print(\"Epoch:\", epoch + 1)\n","\n","    # ---------------------\n","    #  Train Discriminator\n","    # ---------------------\n","\n","    # Select a random batch of real samples\n","    idx = np.random.randint(0, X_train.shape[0], batch_size)\n","    real_samples = y_train[idx]\n","    real_labels = np.ones((batch_size, 1))\n","\n","    # Generate a batch of fake samples\n","    latent_samples = np.random.normal(0, 1, (batch_size, latent_dim))\n","    cond_samples = X_train[idx]\n","    fake_samples = generator_model.predict([latent_samples, cond_samples])\n","    fake_labels = np.zeros((batch_size, 1))\n","\n","    # Train the discriminator\n","    discriminator_loss_real = discriminator_model.train_on_batch(real_samples, real_labels)\n","    discriminator_loss_fake = discriminator_model.train_on_batch(fake_samples, fake_labels)\n","    discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n","\n","    # -----------------\n","    #  Train Generator\n","    # -----------------\n","\n","    # Generate a new batch of latent samples and random condition samples\n","    latent_samples = np.random.normal(0, 1, (batch_size, latent_dim))\n","    cond_samples = X_train[idx]\n","    generator_labels = np.ones((batch_size, 1))\n","\n","    # Train the generator\n","    generator_loss = gan_model.train_on_batch([latent_samples, cond_samples], generator_labels)\n","\n","    # Print the losses\n","    print(\"Discriminator loss:\", discriminator_loss)\n","    print(\"Generator loss:\", generator_loss)\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"LUZtyptQU46S","executionInfo":{"status":"error","timestamp":1688398640074,"user_tz":-330,"elapsed":455,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}},"outputId":"58a06f0a-ae8e-4d93-fbbd-82981edee1f8"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-655a0ac0ec23>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlatent_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcond_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mfake_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mfake_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         return func.predict(\n\u001b[0m\u001b[1;32m   1060\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m     ):\n\u001b[1;32m    797\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         x, _, _ = model._standardize_user_data(\n\u001b[0m\u001b[1;32m    799\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2652\u001b[0;31m         return self._standardize_tensors(\n\u001b[0m\u001b[1;32m   2653\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m             \u001b[0;31m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             x = training_utils_v1.standardize_input_data(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0mfeed_input_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    729\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                     ):\n\u001b[0;32m--> 731\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    732\u001b[0m                             \u001b[0;34m\"Error when checking \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                             \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (100,) but got array with shape (2,)"]}]},{"cell_type":"code","source":["# Training loop\n","epochs = 100  # Set the number of training epochs\n","batch_size = 32  # Set the batch size for training\n","\n","for epoch in range(epochs):\n","    print(\"Epoch:\", epoch + 1)\n","\n","    # ---------------------\n","    #  Train Discriminator\n","    # ---------------------\n","\n","    # Select a random batch of real samples\n","    idx = np.random.randint(0, X_train_processed.shape[0], batch_size)\n","    real_samples = y_train_processed[idx]\n","    real_labels = np.ones((batch_size, 1))\n","\n","    # Generate a batch of fake samples\n","    latent_samples = np.random.normal(0, 1, (batch_size, latent_dim))\n","    cond_samples = X_train_processed[idx]\n","    fake_samples = generator_model.predict([latent_samples, cond_samples])\n","    fake_labels = np.zeros((batch_size, 1))\n","\n","    # Train the discriminator\n","\n","    discriminator_loss_real = discriminator_model.train_on_batch(real_samples, real_labels)\n","    discriminator_loss_fake = discriminator_model.train_on_batch(fake_samples, fake_labels)\n","    discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n","\n","    # -----------------\n","    #  Train Generator\n","    # -----------------\n","\n","    # Generate a new batch of latent samples and random condition samples\n","    latent_samples = np.random.normal(0, 1, (batch_size, latent_dim))\n","    cond_samples = X_train_processed[idx]\n","    generator_labels = np.ones((batch_size, 1))\n","\n","    # Train the generator\n","    generator_loss = gan_model.train_on_batch([latent_samples, cond_samples], generator_labels)\n","\n","    # Print the losses\n","    print(\"Discriminator loss:\", discriminator_loss)\n","    print(\"Generator loss:\", generator_loss)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"ZWTan2nFUoc0","executionInfo":{"status":"error","timestamp":1688398646450,"user_tz":-330,"elapsed":618,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}},"outputId":"9c9427b4-2dd7-4e36-c28c-d60f15cff77c"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-f9b21ccffa9d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlatent_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcond_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_processed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mfake_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mfake_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         return func.predict(\n\u001b[0m\u001b[1;32m   1060\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m     ):\n\u001b[1;32m    797\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         x, _, _ = model._standardize_user_data(\n\u001b[0m\u001b[1;32m    799\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2652\u001b[0;31m         return self._standardize_tensors(\n\u001b[0m\u001b[1;32m   2653\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m             \u001b[0;31m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             x = training_utils_v1.standardize_input_data(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0mfeed_input_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    729\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                     ):\n\u001b[0;32m--> 731\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    732\u001b[0m                             \u001b[0;34m\"Error when checking \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                             \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (100,) but got array with shape (2,)"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","# Plot the losses\n","plt.plot(discriminator_loss, label='Discriminator Loss')\n","plt.plot( generator_loss, label='Generator Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"I6qEruWAF5C0","executionInfo":{"status":"aborted","timestamp":1688397054056,"user_tz":-330,"elapsed":18,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Lists to store discriminator and generator losses\n","discriminator_loss = []\n","generator_loss = []\n","\n","# Training loop\n","epochs = 100\n","batch_size = 32\n","\n","for epoch in range(epochs):\n","    print(\"Epoch:\", epoch + 1)\n","\n","    # ... Training code ...\n","\n","    # Append the losses to the respective lists\n","    discriminator_losses.append(discriminator_loss)\n","    generator_losses.append(generator_loss)\n","\n","    # Print the losses\n","    print(\"Discriminator loss:\", discriminator_loss)\n","    print(\"Generator loss:\", generator_loss)\n","    print()\n","\n","# Create the epoch values (x-values)\n","epochs_values = range(1, epochs + 1)\n","\n","# Repeat the scalar losses to match the dimension of epochs_values\n","discriminator_losses = [discriminator_loss] * epochs\n","generator_losses = [generator_loss] * epochs\n","\n","# Plot the losses\n","plt.plot(epochs_values, discriminator_losses, label='Discriminator Loss')\n","plt.plot(epochs_values, generator_losses, label='Generator Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"FMJ6YVkLIu1F","executionInfo":{"status":"error","timestamp":1688397536318,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}},"outputId":"c49e523c-e87e-470f-9929-e2365d2a5140"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-b95e76fd2f72>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Append the losses to the respective lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdiscriminator_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mgenerator_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vzqVt1UhHDVB","executionInfo":{"status":"aborted","timestamp":1688397054060,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder_input = Input(shape=(latent_dim,))  # shape: (2,)\n","cond_input = Input(shape=(X_train.shape[1],))  # shape: (2,)\n","x = Concatenate(axis=1)([decoder_input, cond_input])  # shape: (4,)\n","x = Dense(12 * 18 * n_filters, activation=activation)(x)  # shape: (6912,)\n","x = Reshape((12, 18, n_filters))(x)  # shape: (12, 18, 32)\n","x = Conv2DTranspose(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x)  # shape: (24, 36, 32)\n","x = Conv2DTranspose(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x)  # shape: (48, 72, 32)\n","x = Conv2DTranspose(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x)  # shape: (96, 144, 32)\n","decoder_output = Conv2DTranspose(1, kernel_size, activation=\"linear\", padding=\"same\")(x)  # shape: (96, 144, 1)\n","\n","generator_model = Model([decoder_input, cond_input], decoder_output, name=\"generator\")\n","generator_model.summary()\n"],"metadata":{"id":"hmjujaFc-QfC","executionInfo":{"status":"aborted","timestamp":1688397054065,"user_tz":-330,"elapsed":25,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator_model = Model([decoder_input, cond_input], decoder_output)\n","discriminator_model = Model(discriminator_inputs, regression_output)\n","\n","# Compile discriminator model\n","discriminator_model.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","# Freeze discriminator weights during GAN training\n","discriminator_model.trainable = False\n","\n","# Combined GAN model\n","gan_input = [discriminator_inputs, cond_input]\n","gan_outputs = discriminator_model(generator_model(gan_input))\n","gan = Model(gan_input, gan_output)\n","\n","# Compile GAN model\n","#gan.compile(loss='binary_crossentropy', optimizer='adam')\n"],"metadata":{"id":"GzSmgqAr-QlL","executionInfo":{"status":"aborted","timestamp":1688397054067,"user_tz":-330,"elapsed":27,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Discriminator model\n","discriminator_input = Input(shape=(n_lat, n_lon, 1))\n","discriminator_output = discriminator_model(discriminator_input)\n","\n","# Combined GAN model\n","gan_input = [discriminator_inputs, cond_input]\n","gan_output = discriminator_model(generator)\n","gan = Model(gan_input, gan_output)\n","\n","# Compile discriminator model\n","discriminator_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Freeze discriminator weights during GAN training\n","discriminator_model.trainable = False\n","\n","# Compile GAN model\n","gan.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","# Print summary of GAN model\n","gan.summary()"],"metadata":{"id":"eHKwoYXm-Qok","executionInfo":{"status":"aborted","timestamp":1688397054070,"user_tz":-330,"elapsed":30,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(2, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        output = self.model(x)\n","        return output"],"metadata":{"id":"V5mnlPYAH_zh","executionInfo":{"status":"aborted","timestamp":1688397054071,"user_tz":-330,"elapsed":31,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator = Discriminator()"],"metadata":{"id":"cye6comEH_2_","executionInfo":{"status":"aborted","timestamp":1688397054073,"user_tz":-330,"elapsed":33,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(2, 16),\n","            nn.ReLU(),\n","            nn.Linear(16, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 2),\n","        )\n","\n","    def forward(self, x):\n","        output = self.model(x)\n","        return output\n","\n","generator = Generator()"],"metadata":{"id":"KFiyjiB2IALt","executionInfo":{"status":"aborted","timestamp":1688397054079,"user_tz":-330,"elapsed":38,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr = 0.001\n","num_epochs = 300\n","loss_function = nn.BCELoss()"],"metadata":{"id":"bnxlMhhhATd2","executionInfo":{"status":"aborted","timestamp":1688397054081,"user_tz":-330,"elapsed":40,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n","optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"],"metadata":{"id":"f1tmXDVrVTUE","executionInfo":{"status":"aborted","timestamp":1688397054083,"user_tz":-330,"elapsed":496053,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_length = 1024\n","train_data = torch.zeros((train_data_length, 2))\n","train_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)\n","train_data[:, 1] = torch.sin(train_data[:, 0])\n","train_labels = torch.zeros(train_data_length)\n","train_set = [\n","    (train_data[i], train_labels[i]) for i in range(train_data_length)\n","]"],"metadata":{"id":"UHmdgIRLVwPQ","executionInfo":{"status":"aborted","timestamp":1688397054084,"user_tz":-330,"elapsed":496047,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SKCGCPfBX7cZ","executionInfo":{"status":"aborted","timestamp":1688397054086,"user_tz":-330,"elapsed":496040,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = tf.convert_to_tensor(X_train)"],"metadata":{"id":"rfWyNofhXBDA","executionInfo":{"status":"aborted","timestamp":1688397054087,"user_tz":-330,"elapsed":496036,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z55kr2yvXTX8","executionInfo":{"status":"aborted","timestamp":1688397054088,"user_tz":-330,"elapsed":496031,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","train_loader = torch.utils.data.DataLoader(\n","    a, batch_size=batch_size, shuffle=True\n",")"],"metadata":{"id":"mPhMD8EFVk4q","executionInfo":{"status":"aborted","timestamp":1688397054089,"user_tz":-330,"elapsed":496016,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-0gdgGmtVWfJ","executionInfo":{"status":"aborted","timestamp":1688397054090,"user_tz":-330,"elapsed":496007,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QHPFWeHpVWi4","executionInfo":{"status":"aborted","timestamp":1688397054091,"user_tz":-330,"elapsed":496005,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eQ16_lXzVWmW","executionInfo":{"status":"aborted","timestamp":1688397054093,"user_tz":-330,"elapsed":496003,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7pINqd1-VTig","executionInfo":{"status":"aborted","timestamp":1688397054097,"user_tz":-330,"elapsed":495999,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":529,"status":"ok","timestamp":1688397176697,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"58KF6-WQXKfg"},"outputs":[],"source":["\n","latent_dim = 2   # dimensions of the latent space\n","n_filters  = 32  # number of filters for the convolutional layers\n","n_neurons  = 32  # number of neurons for the Dense layers\n","activation = 'relu' # activation function\n","kernal_size = 4\n","learning_rate = 0.001\n","batch_size = 64\n","n_epochs     = 200"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1688397179075,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"rN8d5VhyEQSk","outputId":"d63d85ac-c874-4d48-d412-d26a98422937"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"generator\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_19 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_20 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," concatenate_4 (Concatenate)    (None, 4)            0           ['input_19[0][0]',               \n","                                                                  'input_20[0][0]']               \n","                                                                                                  \n"," dense_11 (Dense)               (None, 6912)         34560       ['concatenate_4[0][0]']          \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 12, 18, 32)   0           ['dense_11[0][0]']               \n","                                                                                                  \n"," conv2d_transpose_11 (Conv2DTra  (None, 24, 36, 32)  16416       ['reshape_4[0][0]']              \n"," nspose)                                                                                          \n","                                                                                                  \n"," conv2d_transpose_12 (Conv2DTra  (None, 48, 72, 32)  16416       ['conv2d_transpose_11[0][0]']    \n"," nspose)                                                                                          \n","                                                                                                  \n"," conv2d_transpose_13 (Conv2DTra  (None, 96, 144, 32)  16416      ['conv2d_transpose_12[0][0]']    \n"," nspose)                                                                                          \n","                                                                                                  \n"," conv2d_transpose_14 (Conv2DTra  (None, 96, 144, 1)  513         ['conv2d_transpose_13[0][0]']    \n"," nspose)                                                                                          \n","                                                                                                  \n","==================================================================================================\n","Total params: 84,321\n","Trainable params: 84,321\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_21 (InputLayer)       [(None, 96, 144, 1)]      0         \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 48, 72, 32)        544       \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 24, 36, 32)        16416     \n","                                                                 \n"," conv2d_15 (Conv2D)          (None, 12, 18, 32)        16416     \n","                                                                 \n"," flatten_4 (Flatten)         (None, 6912)              0         \n","                                                                 \n"," dense_12 (Dense)            (None, 32)                221216    \n","                                                                 \n"," dense_13 (Dense)            (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 254,625\n","Trainable params: 254,625\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"gan\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_19 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_20 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," generator (Functional)         (None, 96, 144, 1)   84321       ['input_19[0][0]',               \n","                                                                  'input_20[0][0]']               \n","                                                                                                  \n"," discriminator (Functional)     (None, 1)            254625      ['generator[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 338,946\n","Trainable params: 338,946\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xarray as xr\n","from glob import glob\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import *\n","from tensorflow.keras import Model\n","\n","# Generator\n","#latent_dim = 2\n","#n_filters = 32\n","#kernal_size = 4\n","#n_neurons = 32\n","\n","generator_inputs = Input(shape=(latent_dim,))\n","generator_cond_inputs = Input(shape=(X_train.shape[1],))\n","\n","x = keras.layers.Concatenate(axis=1)([generator_inputs, generator_cond_inputs])\n","x = Dense(12 * 18 * n_filters, activation='relu')(x)\n","x = Reshape((12, 18, n_filters))(x)\n","x = Conv2DTranspose(n_filters, kernal_size, activation='relu', strides=2, padding='same')(x)\n","x = Conv2DTranspose(n_filters, kernal_size, activation='relu', strides=2, padding='same')(x)\n","x = Conv2DTranspose(n_filters, kernal_size, activation='relu', strides=2, padding='same')(x)\n","generator_outputs = Conv2DTranspose(1, kernal_size, activation='linear', padding='same')(x)\n","\n","generator = Model([generator_inputs, generator_cond_inputs], generator_outputs, name='generator')\n","generator.summary()\n","\n","# Discriminator\n","discriminator_inputs = Input(shape=(n_lat, n_lon, 1))\n","\n","x = Conv2D(n_filters, kernal_size, activation='relu', strides=2, padding='same')(discriminator_inputs)\n","x = Conv2D(n_filters, kernal_size, activation='relu', strides=2, padding='same')(x)\n","x = Conv2D(n_filters, kernal_size, activation='relu', strides=2, padding='same')(x)\n","x = Flatten()(x)\n","x = Dense(n_neurons, activation='relu')(x)\n","discriminator_outputs = Dense(1, activation='sigmoid')(x)\n","\n","discriminator = Model(discriminator_inputs, discriminator_outputs, name='discriminator')\n","discriminator.summary()\n","\n","# Define the GAN\n","gan_inputs = [generator_inputs, generator_cond_inputs]\n","gan_outputs = discriminator(generator(gan_inputs))\n","\n","gan = Model(gan_inputs, gan_outputs, name='gan')\n","gan.summary()\n","\n","# Compile the models\n","generator.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n","discriminator.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='binary_crossentropy')\n","gan.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='binary_crossentropy')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4zPzLSAfhet","executionInfo":{"status":"aborted","timestamp":1688397054099,"user_tz":-330,"elapsed":495972,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"outputs":[],"source":["x_real = X_train\n","y_real = y_train\n","fake_samples = X_test\n","fake_labels = y_test\n","n_samples = 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":495963,"status":"aborted","timestamp":1688397054100,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"Ug8L_8C1jIXR"},"outputs":[],"source":[" # Generate fake samples\n","fake_inputs = np.random.normal(size=(n_samples, latent_dim))\n","fake_cond_inputs = np.random.normal(size=(n_samples, X_train.shape[1]))\n","fake_samples = generator.predict([fake_inputs, fake_cond_inputs])\n"]},{"cell_type":"code","source":["# Generate samples from true distribution\n","def generate_samples(n_samples):\n","    x = np.random.randn(n_samples)\n","    y = np.sin(3 * x) + np.random.normal(0, 0.1, n_samples)\n","    return x, y\n","\n","# Generate random noise for generator input\n","def generate_noise(n_samples, latent_dim):\n","    return np.random.randn(n_samples, latent_dim)\n"],"metadata":{"id":"glXDQUzVrXs2","executionInfo":{"status":"aborted","timestamp":1688397054101,"user_tz":-330,"elapsed":495952,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":444,"status":"error","timestamp":1688397185710,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"YVXiOcXXhEof","outputId":"6a1f328c-149a-404b-866e-5dc9d9eded62"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-6297427b387b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Generate real samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m      \u001b[0;31m# Generate fake samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfake_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'generate_samples' is not defined"]}],"source":["for epoch in range(n_epochs):\n","    # Generate real samples\n","    x_real, y_real = generate_samples(n_samples)\n","     # Generate fake samples\n","    fake_inputs = np.random.normal(size=(n_samples, latent_dim))\n","    fake_cond_inputs = np.random.normal(size=(n_samples, X_train.shape[1]))\n","    fake_samples = generator.predict([fake_inputs, fake_cond_inputs])\n","\n","\n","    # Train discriminator\n","    discriminator_loss_real = discriminator.train_on_batch(x_real, np.ones((n_samples,)))\n","    noise = generate_noise(batch_size, latent_dim)\n","    x_fake = generator.predict(noise)\n","    discriminator_loss_fake = discriminator.train_on_batch(x_fake, np.zeros((batch_size,)))\n","    discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n","\n","    # Train generator\n","    noise = generate_noise(batch_size, latent_dim)\n","    generator_loss = gan.train_on_batch(noise, np.ones((batch_size,)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FROg3PbNXTO7","executionInfo":{"status":"aborted","timestamp":1688397054699,"user_tz":-330,"elapsed":19,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H89zeYWda8pX","executionInfo":{"status":"aborted","timestamp":1688397054700,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaRFMkwgkmkT","executionInfo":{"status":"aborted","timestamp":1688397054707,"user_tz":-330,"elapsed":25,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"outputs":[],"source":["latent_dim = 2   # dimensions of the latent space\n","n_filters  = 32  # number of filters for the convolutional layers\n","n_neurons  = 32  # number of neurons for the Dense layers\n","activation = 'relu' # activation function\n","kernal_size = 4\n","learning_rate = 0.001\n","minibatch_size = 64\n","num_epochs     = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1688397054707,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"9gbDir2IkgqV"},"outputs":[],"source":["encoder_inputs = Input(shape=(n_lat, n_lon, 1)) # shape: (96,144,1)\n","x = Conv2D(n_filters, kernal_size, activation=activation, strides=2, padding=\"same\")(encoder_inputs) # shape: (48,72,32)\n","x = Conv2D(n_filters, kernal_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (24,36,32)\n","x = Conv2D(n_filters, kernal_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (12,18,32)\n","x = Flatten()(x) # shape: (6912,1)\n","x = Dense(n_neurons, activation=activation)(x) # shape: (16,1)\n","\n","\n","z_mean    = Dense(latent_dim, name=\"z_mean\")(x) # shape: (2,1)\n","z_log_var = Dense(latent_dim, name=\"z_log_var\")(x) # shape: (2,1)\n","\n","\n","def sample_latent_features(distribution):\n","    \"\"\"\n","    This function takes previously calculated mean & variance,\n","    and returns back a latent encoding vector.\n","    \"\"\"\n","\n","    distribution_mean, distribution_variance = distribution\n","    batch_size = tf.shape(distribution_variance)[0]\n","    random = tf.keras.backend.random_normal(shape=(batch_size, tf.shape(distribution_variance)[1]))\n","    return distribution_mean + tf.exp(0.5 * distribution_variance) * random\n","\n","\n","z         = Lambda(sample_latent_features)([z_mean, z_log_var])\n","\n","encoder_model = Model(encoder_inputs, z, name=\"encoder\")\n","encoder_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"aborted","timestamp":1688397054708,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"wl-m2mIZkjk-"},"outputs":[],"source":["decoder_input = Input(shape=(latent_dim,)) # shape: (2,1)\n","cond_input    = Input(shape=(X_train.shape[1],)) # shape: (2,1)\n","x = keras.layers.Concatenate(axis=1)([decoder_input]) # shape: (4,1)\n","x = Dense(12 * 18 * n_filters, activation=activation)(x) # shape: (6912,1)\n","x = Reshape((12, 18, n_filters))(x) # shape: (12,18,32)\n","x = Conv2DTranspose(n_filters, kernal_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (24,36,32)\n","x = Conv2DTranspose(n_filters, kernal_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (48,72,32)\n","x = Conv2DTranspose(n_filters, kernal_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (96,144,32)\n","decoder_output = Conv2DTranspose(1, kernal_size, activation=\"linear\", padding=\"same\")(x) # shape: (96,144,1)\n","decoder_model = Model([decoder_input,cond_input], decoder_output, name=\"decoder\")\n","decoder_model.summary()"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"1mrXrfsYklHn","executionInfo":{"status":"ok","timestamp":1688397134083,"user_tz":-330,"elapsed":449,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"}}},"outputs":[],"source":["n_filters = 32  # Number of filters in convolutional layers\n","kernel_size = (3, 3)  # Kernel size for convolutional layers\n","activation = \"relu\"  # Activation function for convolutional and dense layers\n","n_neurons = 16  # Number of neurons in the dense layer\n","latent_dim = 2  # Dimensionality of the latent space\n","\n","# Example of X_train.shape for cond_input\n","X_train_shape = (2, 1)  # Replace with the appropriate shape of your training data\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":710,"status":"ok","timestamp":1688397135257,"user":{"displayName":"Anmol chaure","userId":"06805882186255577426"},"user_tz":-330},"id":"OatFPd-1kqa7","outputId":"48ef78d7-3693-491b-9343-c2742ce0fe54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"encoder\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_11 (InputLayer)          [(None, 96, 144, 1)  0           []                               \n","                                ]                                                                 \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 48, 72, 32)   320         ['input_11[0][0]']               \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 24, 36, 32)   9248        ['conv2d_10[0][0]']              \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 12, 18, 32)   9248        ['conv2d_11[0][0]']              \n","                                                                                                  \n"," flatten_3 (Flatten)            (None, 6912)         0           ['conv2d_12[0][0]']              \n","                                                                                                  \n"," dense_7 (Dense)                (None, 16)           110608      ['flatten_3[0][0]']              \n","                                                                                                  \n"," z_mean (Dense)                 (None, 2)            34          ['dense_7[0][0]']                \n","                                                                                                  \n"," z_log_var (Dense)              (None, 2)            34          ['dense_7[0][0]']                \n","                                                                                                  \n"," lambda_1 (Lambda)              (None, 2)            0           ['z_mean[0][0]',                 \n","                                                                  'z_log_var[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 129,492\n","Trainable params: 129,492\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"decoder\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_12 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_13 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 4)            0           ['input_12[0][0]',               \n","                                                                  'input_13[0][0]']               \n","                                                                                                  \n"," dense_8 (Dense)                (None, 6912)         34560       ['concatenate_2[0][0]']          \n","                                                                                                  \n"," reshape_2 (Reshape)            (None, 12, 18, 32)   0           ['dense_8[0][0]']                \n","                                                                                                  \n"," conv2d_transpose_7 (Conv2DTran  (None, 24, 36, 32)  9248        ['reshape_2[0][0]']              \n"," spose)                                                                                           \n","                                                                                                  \n"," conv2d_transpose_8 (Conv2DTran  (None, 48, 72, 32)  9248        ['conv2d_transpose_7[0][0]']     \n"," spose)                                                                                           \n","                                                                                                  \n"," conv2d_transpose_9 (Conv2DTran  (None, 96, 144, 32)  9248       ['conv2d_transpose_8[0][0]']     \n"," spose)                                                                                           \n","                                                                                                  \n"," conv2d_transpose_10 (Conv2DTra  (None, 96, 144, 1)  289         ['conv2d_transpose_9[0][0]']     \n"," nspose)                                                                                          \n","                                                                                                  \n","==================================================================================================\n","Total params: 62,593\n","Trainable params: 62,593\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_16 (InputLayer)       [(None, 96, 144, 1)]      0         \n","                                                                 \n"," encoder (Functional)        (None, 2)                 129492    \n","                                                                 \n"," dense_9 (Dense)             (None, 1)                 3         \n","                                                                 \n","=================================================================\n","Total params: 129,495\n","Trainable params: 3\n","Non-trainable params: 129,492\n","_________________________________________________________________\n","Model: \"model_4\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_14 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," input_15 (InputLayer)          [(None, 2)]          0           []                               \n","                                                                                                  \n"," decoder (Functional)           (None, 96, 144, 1)   62593       ['input_14[0][0]',               \n","                                                                  'input_15[0][0]']               \n","                                                                                                  \n"," discriminator (Functional)     (None, 1)            129495      ['decoder[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 192,088\n","Trainable params: 62,596\n","Non-trainable params: 129,492\n","__________________________________________________________________________________________________\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose, Lambda\n","from tensorflow.keras.models import Model\n","import numpy as np\n","\n","# Encoder\n","encoder_inputs = Input(shape=(n_lat, n_lon, 1)) # shape: (96, 144, 1)\n","x = Conv2D(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(encoder_inputs) # shape: (48, 72, 32)\n","x = Conv2D(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (24, 36, 32)\n","x = Conv2D(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (12, 18, 32)\n","x = Flatten()(x) # shape: (6912,)\n","x = Dense(n_neurons, activation=activation)(x) # shape: (16,)\n","\n","z_mean = Dense(latent_dim, name=\"z_mean\")(x) # shape: (2,)\n","z_log_var = Dense(latent_dim, name=\"z_log_var\")(x) # shape: (2,)\n","\n","def sample_latent_features(distribution):\n","    distribution_mean, distribution_variance = distribution\n","    batch_size = tf.shape(distribution_variance)[0]\n","    random = tf.keras.backend.random_normal(shape=(batch_size, tf.shape(distribution_variance)[1]))\n","    return distribution_mean + tf.exp(0.5 * distribution_variance) * random\n","\n","z = Lambda(sample_latent_features)([z_mean, z_log_var])\n","\n","encoder_model = Model(encoder_inputs, z, name=\"encoder\")\n","encoder_model.summary()\n","\n","\n","# Decoder\n","decoder_input = Input(shape=(latent_dim,)) # shape: (2,)\n","cond_input = Input(shape=(X_train.shape[1],)) # shape: (2,)\n","x = keras.layers.Concatenate(axis=1)([decoder_input, cond_input]) # shape: (4,)\n","x = Dense(12 * 18 * n_filters, activation=activation)(x) # shape: (6912,)\n","x = Reshape((12, 18, n_filters))(x) # shape: (12, 18, 32)\n","x = Conv2DTranspose(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (24, 36, 32)\n","x = Conv2DTranspose(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (48, 72, 32)\n","x = Conv2DTranspose(n_filters, kernel_size, activation=activation, strides=2, padding=\"same\")(x) # shape: (96, 144, 32)\n","decoder_output = Conv2DTranspose(1, kernel_size, activation=\"linear\", padding=\"same\")(x) # shape: (96, 144, 1)\n","decoder_model = Model([decoder_input, cond_input], decoder_output, name=\"decoder\")\n","decoder_model.summary()\n","\n","\n","# GAN (Generator + Discriminator)\n","gan_input = Input(shape=(latent_dim,))\n","gan_cond_input = Input(shape=(X_train.shape[1],))\n","generated_image = decoder_model([gan_input, gan_cond_input])\n","\n","# Freeze the weights of the encoder during GAN training\n","encoder_model.trainable = False\n","\n","# Discriminator\n","discriminator_input = Input(shape=(n_lat, n_lon, 1))\n","discriminator_features = encoder_model(discriminator_input)\n","discriminator_output = Dense(1, activation=\"sigmoid\")(discriminator_features)\n","\n","discriminator_model = Model(discriminator_input, discriminator_output, name=\"discriminator\")\n","discriminator_model.summary()\n","\n","# Compile the discriminator\n","discriminator_model.compile(optimizer=keras.optimizers.Adam(), loss=\"binary_crossentropy\")\n","\n","# Combined GAN model (Generator + Discriminator)\n","gan_output = discriminator_model(generated_image)\n","gan = Model([gan_input, gan_cond_input], gan_output)\n","gan.summary()\n","\n","# Compile the GAN\n","gan.compile(optimizer=keras.optimizers.Adam(), loss=\"binary_crossentropy\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyMlqAvaM8mMW8ETwq6nxxKU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}